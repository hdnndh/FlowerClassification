# -*- coding: utf-8 -*-
"""ECET380_Assgn2_Part1_efficientdetb0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VLhBVnpsB5mQTiC2vUB2Tmprm3gJiY9K
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload

import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
import os
import preprocessDefinition
import importlib
!pip install -q -U keras-tuner
import kerastuner as kt
importlib.reload(preprocessDefinition)
from preprocessDefinition import preprocess
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

dataset, info = tfds.load("oxford_flowers102", with_info=True, as_supervised=True)
dataset

info.features

class_names = info.features["label"].names
class_names

n_classes = info.features['label'].num_classes
n_classes

dataset_size = info.splits['train'].num_examples
dataset_size

dataset['train']

train_set   = tfds.load('oxford_flowers102', split='train', as_supervised=True)
val_set     = tfds.load('oxford_flowers102', split='validation', as_supervised=True)
test_set     = tfds.load('oxford_flowers102', split='test', as_supervised=True)

batch_size = 32
train_set = train_set.shuffle(1000).repeat()
train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)
val_set = val_set.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)

base_model = keras.applications.efficientnet.EfficientNetB0(
        weights = "imagenet",
        include_top = False)

def model_builder(hp):
    for layer in base_model.layers:
        layer.trainable = False

    hp_learning_rate = hp.Choice(
        'learning_rate', 
        values = [0.3, 0.2, 0.1])
    
    hp_momentum = hp.Choice(
        'momentum', 
        values = [0.5, 0.7, 0.9])
    
    hp_decay = hp.Choice(
        'decay', 
        values = [0.05, 0.01, 0.005])
    
    avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
    output = keras.layers.Dense(
        units = n_classes, 
        activation = "softmax")(avg)

    model = keras.models.Model(
        inputs = base_model.input,
        outputs = output)

    model.compile(
        optimizer = keras.optimizers.SGD(
            learning_rate = hp_learning_rate,
            momentum=hp_momentum,
            decay=hp_decay
            ),
        loss = "sparse_categorical_crossentropy", 
        metrics = ['accuracy'])
    
    model.summary()
    return model

tuner = kt.RandomSearch(
            model_builder,
            objective='val_accuracy',
            max_trials=27,
            seed=0,
            overwrite=True,
            executions_per_trial=2)

Early = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)
Checkpoint = keras.callbacks.ModelCheckpoint("flowersModel.h5", save_best_only=True)

tuner.search(train_set,
            steps_per_epoch=int(0.75 * dataset_size / batch_size),
            validation_data=val_set,
            validation_steps=int(0.15 * dataset_size / batch_size),
            epochs=40,
            callbacks=[Early]
            )

best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]

model_2 = tuner.hypermodel.build(best_hps)
best_hps

optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.001)
for layer in model_2.layers:
    layer.trainable = True
model_2.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])

history = model_2.fit(train_set,
    steps_per_epoch=int(0.75 * dataset_size / batch_size),
    validation_data=val_set,
    validation_steps=int(0.15 * dataset_size / batch_size),
    epochs=64,
    callbacks=[Early, Checkpoint]
    )

fig_acc = plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='lower right')

model_2.evaluate(test_set)

model=tf.keras.models.load_model('flowersModel.h5')
evalset,info = tfds.load(name='oxford_flowers102', split='test',as_supervised=True,with_info=True)
evalPipe=evalset.map(preprocess,num_parallel_calls=16).batch(128).prefetch(1)
for feats,lab in evalPipe.unbatch().batch(6000).take(1):
	probPreds=model.predict(feats)

top1err=tf.reduce_mean(keras.metrics.sparse_top_k_categorical_accuracy(lab,probPreds,k=1))
top5err=tf.reduce_mean(keras.metrics.sparse_top_k_categorical_accuracy(lab,probPreds,k=5))
top10err=tf.reduce_mean(keras.metrics.sparse_top_k_categorical_accuracy(lab,probPreds,k=10))
print(top10err, top5err, top1err)

